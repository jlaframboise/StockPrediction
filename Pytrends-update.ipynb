{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "collect-encyclopedia",
   "metadata": {},
   "source": [
    "Pytrends implementation - a warning: do not build too many payloads as Google will block your ip address and think you are a bot that's trying to spam its systems (error 429). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "variable-generation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from dateutil.relativedelta import *\n",
    "from dateutil.rrule import *\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-rapid",
   "metadata": {},
   "source": [
    "Timeframe setup. A timeframe over 9 months will condense the data into weekly markers - not very good for our use. Timeframes are 9 months so they are still daily markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "elect-raleigh",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_date = date(2016,1,1)\n",
    "today = date.today()\n",
    "#beginning period of 9 months\n",
    "begin = list(rrule(freq=MONTHLY, interval=8,count=10, dtstart=use_date))\n",
    "for i in range(len(begin)):\n",
    "    begin[i] = begin[i].date()\n",
    "\n",
    "#calculate the last day of last month\n",
    "use_date = use_date+relativedelta(months=-1)\n",
    "use_date = use_date+relativedelta(day=31)\n",
    "\n",
    "#end period of 9 months\n",
    "end = list(rrule(freq=MONTHLY, interval=8,count=11, dtstart=use_date, bymonthday=(-1,)))\n",
    "for i in range(len(end)):\n",
    "    end[i] = end[i].date()\n",
    "\n",
    "#adds time periods to a list\n",
    "#if the end date is later than today's date, today's date will be used\n",
    "dates = []\n",
    "for i in range(len(begin)):\n",
    "    if today >= begin[i] and today >= end[i+1]:\n",
    "        full = (str(begin[i]), str(end[i+1]))\n",
    "        dates.append(full)\n",
    "    elif today >= begin[i] and today <= end[i+1]:\n",
    "        recent = (str(begin[i]), str(today))\n",
    "        dates.append(recent)\n",
    "for i in dates:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-bernard",
   "metadata": {},
   "source": [
    "Data processing. Creates a 'data.csv' file to store the data for each 9 month period while mantaining its daily marker. The keywords come from 'keywords.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressed-superior",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'keywords.txt'\n",
    "with open(filename) as f:\n",
    "    content = f.readlines()\n",
    "for i in range(len(content)):\n",
    "    content[i] = content[i].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-lloyd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "swedish-czech",
   "metadata": {},
   "source": [
    "Constructing the payload with the keywords lists. After constructing the payload, it will be written to a csv file with the name of 'data_.csv' where the _ is the keyword. It will iterate through the keywords, then iterate through each 9 month time period so it maintains its daily markers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytrend = TrendReq()\n",
    "csvname = 'data'\n",
    "allcsv = []\n",
    "for i in content: \n",
    "    kw = []\n",
    "    kw.append(i)\n",
    "    csvfile = csvname + ' ' + i + '.csv'\n",
    "    allcsv.append(csvfile)\n",
    "    for j in dates:\n",
    "        timef = ''\n",
    "        timef = j[0] + ' ' +  j[1]           \n",
    "        time.sleep(5) #if 0, or removed google will think you are a bot so make sure it's greater than 1\n",
    "        pytrend.build_payload(kw,timeframe=timef)\n",
    "        df = pytrend.interest_over_time().drop(['isPartial'],axis=1)\n",
    "        try:\n",
    "            if os.stat(csvfile).st_size > 0:\n",
    "                df.to_csv(csvfile, mode='a', header=False)\n",
    "            else:\n",
    "                #file should never be empty\n",
    "                print(\"empty file\")\n",
    "        except OSError:\n",
    "            df.to_csv(csvfile,index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-spokesman",
   "metadata": {},
   "outputs": [],
   "source": [
    "allcsvadj = []\n",
    "for i in content:\n",
    "    csvfile = csvname + ' ' + i + ' adjusted'+'.csv'\n",
    "    colname = i + ' adjusted'\n",
    "    allcsvadj.append(csvfile)\n",
    "    print([i])\n",
    "    for j in dates:\n",
    "        overlap_start = None\n",
    "        start = datetime.strptime(j[0], '%Y-%m-%d')\n",
    "        initial_end = end = datetime.strptime(j[1], '%Y-%m-%d')\n",
    "        temp_days = end - start        \n",
    "        delta = timedelta(days=temp_days.days)\n",
    "        overlap = timedelta(days=100) # number of days to overlap, might need to increase/perform some fine tuning\n",
    "        iterate = end - delta\n",
    "        df = pd.DataFrame()\n",
    "        ol = pd.DataFrame()\n",
    "        while end > start:\n",
    "            timef = iterate.strftime('%Y-%m-%d')+' '+end.strftime('%Y-%m-%d')\n",
    "            pytrend.build_payload([i],timeframe=timef)\n",
    "            temp = pytrend.interest_over_time().drop(['isPartial'],axis=1)\n",
    "            temp.columns.values[0] = timef\n",
    "            ol_temp =temp.copy()\n",
    "            ol_temp.iloc[:,:] = None\n",
    "            if overlap_start is not None: # after first iteration\n",
    "                y1 = temp.loc[overlap_start:end].iloc[:,0].values.max()\n",
    "                y2 = df.loc[overlap_start:end].iloc[:,-1].values.max()\n",
    "                coef = y2/y1 # coefficent to multiply by for scaling\n",
    "                temp = temp * coef\n",
    "                ol_temp.loc[overlap_start:end, :] = 1 \n",
    "            df = pd.concat([df,temp],axis = 1)\n",
    "            ol = pd.concat([ol,ol_temp],axis = 1)\n",
    "            # shift the timeframe\n",
    "            overlap_start = iterate\n",
    "            end -= (delta-overlap)\n",
    "            iterate -= (delta-overlap)\n",
    "            time.sleep(2) #if 0, or removed google will think you are a bot so make sure it's greater than 1\n",
    "        # averages for periods that were overlapped \n",
    "        df = df.mean(axis=1)\n",
    "        ol = ol.max(axis=1)\n",
    "        # merge the two dataframe (trend data and overlap flag)\n",
    "        df = pd.concat([df,ol], axis=1)\n",
    "        df.columns = [colname,'overlap'] #overlap value isn't useful (1.0 or NaN)\n",
    "        df = df[start:intitial_end]\n",
    "        # rescale to 100 (optional)\n",
    "        df[colname] = (100*df[colname]/df[colname].max()).round(decimals=0)\n",
    "        df = df.drop(['overlap'],axis=1)\n",
    "        try:\n",
    "            if os.stat(csvfile).st_size > 0:\n",
    "                df.to_csv(csvfile, mode='a', header=False)\n",
    "            else:\n",
    "                #file should never be empty\n",
    "                print(\"empty file\")\n",
    "        except OSError:\n",
    "            df.to_csv(csvfile,index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-terror",
   "metadata": {},
   "source": [
    "Each pair of csvs (i.e. initial climate change and ajdusted climate change csvs) are then concatenated into one big csv. Extra date columns are removed as they are redundant. (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-station",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is optional\n",
    "count = 0\n",
    "for i in range(len(allcsv)):\n",
    "    allData = []\n",
    "    df = pd.read_csv(allcsv[i])\n",
    "    df1 = pd.read_csv(allcsvadj[i])\n",
    "    if count > 0:\n",
    "        df = df.drop('date',axis=1)\n",
    "        df1 = df1.drop('date',axis=1)\n",
    "    allData.append(df)\n",
    "    allData.append(df1)\n",
    "    count +=1\n",
    "    filename = 'aggregate' + allcsv[i] + '.csv'\n",
    "    compare = pd.concat(allData, axis=1)\n",
    "    compare.to_csv(filename,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
